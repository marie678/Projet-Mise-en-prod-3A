[
  {
    "objectID": "README_new_features.html",
    "href": "README_new_features.html",
    "title": "Changes with the â€˜Mise en productionâ€™ course",
    "section": "",
    "text": "Original project structure\nCurrent project structure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal repository architecture\nCurrent repository architecture\n\n\n\n\n\n\n\n\n\n\n\nThe new repository architecture was simplified:\n\nInsted of giving acces to older versions of the app via a â€œdevâ€ folder in the repository, we used tags which allow us to retrace our steps and at the same time make the application architecture visible in root.\nThere are no longer a separation between data, preprocessing and application folders. This also enables better visibility and faster understanding of the project.\nA unique README now summurises all the application functionnalities and characteristics.\n\n\n\n\nInstead of doing the preprocessing locally on our computers, using the data sets downloaded from kaggle, and then saving the pre processed dataset in Github, we now do the preprocessing as part of the application code using the original data frames in an s3 folder.\nWe also directly use parquet files to do load and process the data wich saves time and memory. Indeed,\n\n\n\nData set\n\n\nMinimum loading time - memory\n\n\n\n\n\n\nCSV\n\n\nParquet\n\n\nOptimized parquet\n\n\n\n\nRecipes\n\n\n16 s - 687k Ko\n\n\n7 s - 174k Ko\n\n\n6 s - 91963124 bites / 11.5k Ko\n\n\n\n\nrecipes_data\n\n\n65 s - 1 256k Ko\n\n\n40 s - 986k Ko\n\n\n36 s - 89245812 bites / 11k Ko\n\n\n\nThe preprocessing code was adapted to reduce the compute time.\n\n\n\n\nThe quality of the code was improved and tested (imports in order, pep8 coding conventions, typing, â€¦).\nMore functions were created and put in a unique structured folder. Rather than having them in a unique script we created 2 folders and 6 scripts with different functionality for more visibility.\nLoad/create the preprocessed dataset when the app is opened for the first time.\nExternal parametrizaion with YAML file: the dataframes related parameters (columns to keep or format, file names) and s3 connexion parameters were externalized.\n\n\n\n\n\nCreated CI tests to continuously test the code quality and run the unit tests + to continuously deploy a Docker image of the app to Dockerhub\nDeployed manually via kubernetes\nThen created a CD pipeline to deploy automatically via ArgoCD"
  },
  {
    "objectID": "README_new_features.html#repository-organisation-architecture",
    "href": "README_new_features.html#repository-organisation-architecture",
    "title": "Changes with the â€˜Mise en productionâ€™ course",
    "section": "",
    "text": "The new repository architecture was simplified:\n\nInsted of giving acces to older versions of the app via a â€œdevâ€ folder in the repository, we used tags which allow us to retrace our steps and at the same time make the application architecture visible in root.\nThere are no longer a separation between data, preprocessing and application folders. This also enables better visibility and faster understanding of the project.\nA unique README now summurises all the application functionnalities and characteristics."
  },
  {
    "objectID": "README_new_features.html#data-externalisation",
    "href": "README_new_features.html#data-externalisation",
    "title": "Changes with the â€˜Mise en productionâ€™ course",
    "section": "",
    "text": "Instead of doing the preprocessing locally on our computers, using the data sets downloaded from kaggle, and then saving the pre processed dataset in Github, we now do the preprocessing as part of the application code using the original data frames in an s3 folder.\nWe also directly use parquet files to do load and process the data wich saves time and memory. Indeed,\n\n\n\nData set\n\n\nMinimum loading time - memory\n\n\n\n\n\n\nCSV\n\n\nParquet\n\n\nOptimized parquet\n\n\n\n\nRecipes\n\n\n16 s - 687k Ko\n\n\n7 s - 174k Ko\n\n\n6 s - 91963124 bites / 11.5k Ko\n\n\n\n\nrecipes_data\n\n\n65 s - 1 256k Ko\n\n\n40 s - 986k Ko\n\n\n36 s - 89245812 bites / 11k Ko\n\n\n\nThe preprocessing code was adapted to reduce the compute time."
  },
  {
    "objectID": "README_new_features.html#code",
    "href": "README_new_features.html#code",
    "title": "Changes with the â€˜Mise en productionâ€™ course",
    "section": "",
    "text": "The quality of the code was improved and tested (imports in order, pep8 coding conventions, typing, â€¦).\nMore functions were created and put in a unique structured folder. Rather than having them in a unique script we created 2 folders and 6 scripts with different functionality for more visibility.\nLoad/create the preprocessed dataset when the app is opened for the first time.\nExternal parametrizaion with YAML file: the dataframes related parameters (columns to keep or format, file names) and s3 connexion parameters were externalized."
  },
  {
    "objectID": "README_new_features.html#application",
    "href": "README_new_features.html#application",
    "title": "Changes with the â€˜Mise en productionâ€™ course",
    "section": "",
    "text": "Created CI tests to continuously test the code quality and run the unit tests + to continuously deploy a Docker image of the app to Dockerhub\nDeployed manually via kubernetes\nThen created a CD pipeline to deploy automatically via ArgoCD"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Frigo Vide - Recipe Recommender",
    "section": "",
    "text": "View on GitHub\n\nGo to the website\n\n\nğŸ¯ Project Objective\nThis project provides a smart recipe recommender system that helps users cook meals with whatâ€™s left in their fridge. The system analyzes ingredients and suggests recipes using natural language queries and data processing pipelines.\n\n\nğŸ” Project Features\n\nSearch Recipes by ingredients or free-text queries\nData Cleaning & Filtering to ensure high-quality recipe suggestions\nStreamlit App for an interactive user experience\nKubernetes Deployment with CI/CD using ArgoCD\nDockerized Pipeline for reproducibility and scalability\n\n\n\nğŸš€ Technologies Used\n\nPython\nStreamlit\nKubernetes (Minikube)\nGitHub Actions (CI)\nArgoCD (CD)\nDocker\nQuarto for documentation\n\n\n\nğŸ“¦ Dataset\nThe dataset contains nutritional and measurement data for thousands of recipes and is preprocessed before being used in the app.\n\n\nğŸŒ Access the App\nYou can access the deployed app here: frigo-vide-test1.lab.sspcloud.fr\nYou can access the GitHub repository here: https://github.com/marie678/Projet-Mise-en-prod-3A.git\n\n\nğŸ› ï¸ Project Structure\n.\nâ”œâ”€â”€ .github/workflows/\nâ”œâ”€â”€ .streamlit/\nâ”œâ”€â”€ app/\nâ”œâ”€â”€ assets\nâ”‚ â”œâ”€â”€ css/\nâ”‚ â”œâ”€â”€ html/\nâ”‚ â”œâ”€â”€ images/\nâ”‚ â””â”€â”€ js/\nâ”œâ”€â”€ data/\nâ”œâ”€â”€ deployment/\nâ”œâ”€â”€ pages/\nâ”œâ”€â”€ src/\nâ”‚ â”œâ”€â”€ app\nâ”‚ â””â”€â”€ preprocessing\nâ”œâ”€â”€ tests\nâ”œâ”€â”€ utils\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ requirements.txt\nâ””â”€â”€ README.md\n\n\nâœï¸ Authors\n\nMarie Meyer â€“ GitHub\nNoÃ©mie GuibÃ© - GitHub\n\n\n\n\n\n\n Back to top"
  }
]