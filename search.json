[
  {
    "objectID": "README_new_features.html",
    "href": "README_new_features.html",
    "title": "Changes with the â€˜Mise en productionâ€™ course",
    "section": "",
    "text": "The following general project and more detailed repository structure graphs illustrate how the project has evolved from its previous version. They highlight changes in architecture, data handling, and code structure. For more details on these improvements, refer to the sections below:\n\n\n\n\n\n\n\nOriginal project structure\nCurrent project structure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal repository architecture\nCurrent repository architecture\n\n\n\n\n\n\n\n\n\n\n\nThe new repository architecture was simplified:\n\nInstead of giving access to older versions of the app via a â€œdevâ€ folder in the repository, we used tags which allow us to retrace our steps and at the same time make the application architecture visible in root.\nThere is no longer a separation between data, preprocessing and application folders. This also enables better visibility and faster understanding of the project.\nA unique README now summarizes all the application functionalities and characteristics.\n\n\n\n\nInstead of doing the preprocessing locally on our computers, using the datasets downloaded from kaggle and then saving the pre processed dataset in Github, we now do the preprocessing as part of the application code using the original dataframes in an s3 folder.\nWe also directly use parquet files to load and process the data which saves time and memory. Indeed,\n\n\n\nData set\n\n\nMinimum loading time - memory\n\n\n\n\n\n\nCSV\n\n\nParquet\n\n\nOptimized parquet\n\n\n\n\nRecipes\n\n\n16 s - 687k Ko\n\n\n7 s - 174k Ko\n\n\n6 s - 91963124 bites / 11.5k Ko\n\n\n\n\nrecipes_data\n\n\n65 s - 1 256k Ko\n\n\n40 s - 986k Ko\n\n\n36 s - 89245812 bites / 11k Ko\n\n\n\nThe preprocessing code was adapted to reduce the compute time.\n\n\n\n\nThe quality of the code was improved and tested (imports in order, pep8 coding conventions, typing, â€¦).\nMore functions were created and put in a unique structured folder. Rather than having them in a unique script we created 3 folders and 11 scripts with different functionality for more visibility.\nLoad/create the preprocessed dataset when the app is opened for the first time only.\nExternal parametrization with YAML file: the dataframes related parameters (columns to keep or format, file names) and s3 connexion parameters were externalized.\nAdditional logging was added throughout the codebase to help track the applicationâ€™s state and performance. In particular, the entire preprocessing is now timed step by step, making it easier to identify bottlenecks and optimize performance when needed.\n\n\n\n\nA new set of user-related features was added to enhance personalization and interactivity:\n\nUser authentication: Users can now register and log in to the app. Authentication is handled via a Flask backend API, compatible with the Streamlit frontend.\nRecipe liking: Logged-in users can like recipes, which are then saved to their personal user space for later access.\nDatabase integration:\n\nA Users table stores authentication information.\nA Likes table stores the recipes liked by users.\n\n\nBoth databases are implemented using SQLAlchemy for seamless integration with the backend API.\nThis new layer of functionality lays the groundwork for future personalization features such as saved preferences, history tracking, and personalized recommendations. While we didnâ€™t have time to implement those additional features yet, the most technically challenging part is now done.\n\n\n\n\nCreated CI tests to continuously test the code quality and run the unit tests + to continuously deploy a Docker image of the app to Dockerhub\nDeployed manually via kubernetes\nThen created a CD pipeline to deploy automatically via ArgoCD"
  },
  {
    "objectID": "README_new_features.html#repository-organisation-architecture",
    "href": "README_new_features.html#repository-organisation-architecture",
    "title": "Changes with the â€˜Mise en productionâ€™ course",
    "section": "",
    "text": "The new repository architecture was simplified:\n\nInstead of giving access to older versions of the app via a â€œdevâ€ folder in the repository, we used tags which allow us to retrace our steps and at the same time make the application architecture visible in root.\nThere is no longer a separation between data, preprocessing and application folders. This also enables better visibility and faster understanding of the project.\nA unique README now summarizes all the application functionalities and characteristics."
  },
  {
    "objectID": "README_new_features.html#data-externalisation",
    "href": "README_new_features.html#data-externalisation",
    "title": "Changes with the â€˜Mise en productionâ€™ course",
    "section": "",
    "text": "Instead of doing the preprocessing locally on our computers, using the datasets downloaded from kaggle and then saving the pre processed dataset in Github, we now do the preprocessing as part of the application code using the original dataframes in an s3 folder.\nWe also directly use parquet files to load and process the data which saves time and memory. Indeed,\n\n\n\nData set\n\n\nMinimum loading time - memory\n\n\n\n\n\n\nCSV\n\n\nParquet\n\n\nOptimized parquet\n\n\n\n\nRecipes\n\n\n16 s - 687k Ko\n\n\n7 s - 174k Ko\n\n\n6 s - 91963124 bites / 11.5k Ko\n\n\n\n\nrecipes_data\n\n\n65 s - 1 256k Ko\n\n\n40 s - 986k Ko\n\n\n36 s - 89245812 bites / 11k Ko\n\n\n\nThe preprocessing code was adapted to reduce the compute time."
  },
  {
    "objectID": "README_new_features.html#code",
    "href": "README_new_features.html#code",
    "title": "Changes with the â€˜Mise en productionâ€™ course",
    "section": "",
    "text": "The quality of the code was improved and tested (imports in order, pep8 coding conventions, typing, â€¦).\nMore functions were created and put in a unique structured folder. Rather than having them in a unique script we created 3 folders and 11 scripts with different functionality for more visibility.\nLoad/create the preprocessed dataset when the app is opened for the first time only.\nExternal parametrization with YAML file: the dataframes related parameters (columns to keep or format, file names) and s3 connexion parameters were externalized.\nAdditional logging was added throughout the codebase to help track the applicationâ€™s state and performance. In particular, the entire preprocessing is now timed step by step, making it easier to identify bottlenecks and optimize performance when needed."
  },
  {
    "objectID": "README_new_features.html#user-functionality",
    "href": "README_new_features.html#user-functionality",
    "title": "Changes with the â€˜Mise en productionâ€™ course",
    "section": "",
    "text": "A new set of user-related features was added to enhance personalization and interactivity:\n\nUser authentication: Users can now register and log in to the app. Authentication is handled via a Flask backend API, compatible with the Streamlit frontend.\nRecipe liking: Logged-in users can like recipes, which are then saved to their personal user space for later access.\nDatabase integration:\n\nA Users table stores authentication information.\nA Likes table stores the recipes liked by users.\n\n\nBoth databases are implemented using SQLAlchemy for seamless integration with the backend API.\nThis new layer of functionality lays the groundwork for future personalization features such as saved preferences, history tracking, and personalized recommendations. While we didnâ€™t have time to implement those additional features yet, the most technically challenging part is now done."
  },
  {
    "objectID": "README_new_features.html#application",
    "href": "README_new_features.html#application",
    "title": "Changes with the â€˜Mise en productionâ€™ course",
    "section": "",
    "text": "Created CI tests to continuously test the code quality and run the unit tests + to continuously deploy a Docker image of the app to Dockerhub\nDeployed manually via kubernetes\nThen created a CD pipeline to deploy automatically via ArgoCD"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Frigo Vide - Recipe Recommender",
    "section": "",
    "text": "View on GitHub\nGo to the website\n\n\nğŸ¯ Project Objective\nThis project provides a smart recipe recommender system that helps users cook meals with whatâ€™s left in their fridge. The system analyzes ingredients and suggests recipes using natural language queries and data processing pipelines.\n\n\nğŸ” Project Features\n\nSearch Recipes by ingredients or free-text queries\nData Cleaning & Filtering to ensure high-quality recipe suggestions\nStreamlit App for an interactive user experience\nKubernetes Deployment with CI/CD using ArgoCD\nDockerized Pipeline for reproducibility and scalability\n\n\n\nğŸš€ Technologies Used\n\nPython\nStreamlit\nKubernetes (Minikube)\nGitHub Actions (CI)\nArgoCD (CD)\nDocker\nQuarto for documentation\n\n\n\nğŸ“¦ Dataset\nThe dataset contains nutritional and measurement data for thousands of recipes and is preprocessed before being used in the app.\n\n\nğŸŒ Access the App\nYou can access the deployed app here: frigo-vide.lab.sspcloud.fr\nYou can access the GitHub repository here: https://github.com/marie678/Projet-Mise-en-prod-3A.git\n\n\nğŸ› ï¸ Project Structure\n.\nâ”œâ”€â”€ .github/workflows/\nâ”œâ”€â”€ .streamlit/\nâ”œâ”€â”€ app/\nâ”œâ”€â”€ assets\nâ”‚ â”œâ”€â”€ css/\nâ”‚ â”œâ”€â”€ html/\nâ”‚ â”œâ”€â”€ images/\nâ”‚ â””â”€â”€ js/\nâ”œâ”€â”€ data/\nâ”œâ”€â”€ deployment/\nâ”œâ”€â”€ pages/\nâ”œâ”€â”€ src/\nâ”‚ â”œâ”€â”€ app\nâ”‚ â””â”€â”€ preprocessing\nâ”œâ”€â”€ tests\nâ”œâ”€â”€ utils\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ requirements.txt\nâ””â”€â”€ README.md\n\n\nâœï¸ Authors\n\nMarie Meyer â€“ GitHub\nNoÃ©mie GuibÃ© - GitHub\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data/recipe/data_report.html",
    "href": "data/recipe/data_report.html",
    "title": "Dynamic Plotly Charts",
    "section": "",
    "text": "Data Overview\nTo get a clearer understanding of the datasetâ€™s structure, we start with a brief interactive preview.\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.3.0\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\nOur final processed recipe dataset contains 44289 recipes, each described by a wide range of characteristics reflected in the dataset's columns.\n\n\n\n\n\n\n    \n      \n      AuthorName\n      CookTime\n      PrepTime\n      TotalTime\n      Description\n      Images\n      RecipeCategory\n      Keywords\n      AggregatedRating\n      ReviewCount\n      Calories\n      FatContent\n      SaturatedFatContent\n      CholesterolContent\n      SodiumContent\n      CarbohydrateContent\n      FiberContent\n      SugarContent\n      ProteinContent\n      RecipeServings\n      title\n      directions\n      ingredients\n      link\n      NER\n      CookTime_minutes\n      PrepTime_minutes\n      TotalTime_minutes\n      recipe_id\n      TotalTime_cat\n      RecipeType\n      Beginner_Friendly\n      Vegetarian_Friendly\n      World_Cuisine\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\n\n\nInteractive Visualizations\nOur final processed dataset also covers a wide variety of types in terms of origin, preparation time, nutritional content. You can explore these features interactively using the dropdown menu below to browse through the different columns.\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\n\n Back to top"
  }
]